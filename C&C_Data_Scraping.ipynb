{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import concurrent.futures\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Requests Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use the Python 'requests' library to make calls to the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes a 'GET' request to the yahoo, should return '200'\n",
    "response = requests.get(\"https://finance.yahoo.com/quote/TSLA?p=TSLA\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response header (metadata about response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response header\n",
    "header = response.headers\n",
    "# print each header item\n",
    "for index, item in enumerate(header):\n",
    "    print(f\"{index} {item} -- {header[item]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### '.text' method gives us the page source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page source\n",
    "page_text = response.text\n",
    "# notices that requests returns the page as a 'str'\n",
    "print(type(page_text))\n",
    "print(page_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'beautifulsoup' module allows us to search the webpage by tag/selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create BeautifulSoup Object\n",
    "source = bs.BeautifulSoup(page_text)\n",
    "# notices that 'source' is a BeautifulSoup object\n",
    "print(type(source))\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds all elements with the \"a\" tag\n",
    "links = source.find_all('a')\n",
    "for link in links:\n",
    "    print(link)\n",
    "    # note each element in list is a BuutifulSoup 'element tag' object\n",
    "    print(type(link))\n",
    "    print('--', link['href'])\n",
    "    print(\"–\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finds all elements with the \"a\" tag, prints \n",
    "links = source.find_all('a')\n",
    "for link in links:\n",
    "    print(link.text)\n",
    "    print(\"–\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Get Company Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# takes any ticker symbol and returs financial stats as a dict\n",
    "def get_company_data(ticker: str) -> dict:\n",
    "    '''\n",
    "    Parameters: A ticker symbol (str)\n",
    "    Returns: A dict of financial data\n",
    "    '''\n",
    "    # base url for yahoo financial stats\n",
    "    url = f'https://finance.yahoo.com/quote/{ticker}/key-statistics?p={ticker}'\n",
    "    headers = {'User-Agent': \"Mozilla/5.0\"}\n",
    "    \n",
    "    # makes request \n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    \n",
    "    # handles for bad url\n",
    "    if response.status_code != 200:\n",
    "        return {\"ticker\": ticker, \"!status\": f'code {response.status_code}'}\n",
    "    \n",
    "    # main bs page object\n",
    "    source = bs.BeautifulSoup(response.text)\n",
    "    data = source.find('section', {\"data-test\":\"qsp-statistics\"})\n",
    "    \n",
    "    # handles for invalid ticker symbol, \".find()\" returns \"None\" \n",
    "    if data == None:\n",
    "        return {\"ticker\": ticker, \"!status\": \"data == None\"}\n",
    "    # finds the company name by id and h1 tag\n",
    "    company_name = source.find('div', {'id':'quote-header-info'}).find('h1').text\n",
    "    \n",
    "    # creates a list of all 'tr' elements\n",
    "    rows = data.find_all('tr')\n",
    "\n",
    "    # dict comprehension\n",
    "    #info_dict = {row.find_all('td')[0].text: row.find_all('td')[1].text for row in rows}\n",
    "    \n",
    "    info_dict = {\"ticker\": ticker, \"!status\": \"good\", '!!company_name': company_name}\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "        key = data[0].text\n",
    "        value = data[1].text\n",
    "        info_dict[key] = value\n",
    "\n",
    "    return info_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_company_data.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = ['tsla', 'goog', 'gs']\n",
    "df = pd.DataFrame(list(map(get_company_data, ticker_list)))\n",
    "df = df.set_index('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df.index)} Rows, {len(df.columns)} Columns\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Get List of S&P 500 Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of S&P 500 tickers from Wikipedia,\n",
    "# returns dictionary of company ticker and name\n",
    "def get_tickers() -> list:\n",
    "    '''\n",
    "    Parameters: None\n",
    "    Returns: A list of dictionaries containing companies and their tickers\n",
    "    '''\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    \n",
    "    # bs object, searchable by dom elements\n",
    "    source = bs.BeautifulSoup(response.text)    \n",
    "    main_table = source.find('table', {\"id\": \"constituents\"})\n",
    "    table_body = main_table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "    # a list of to elements dictionaries\n",
    "    company_list = []\n",
    "    for row in rows:\n",
    "        # row contains list of td (table data) elements\n",
    "        row_cells = row.find_all('td')\n",
    "        # skips any row missing esential data (first and second column)\n",
    "        if len(row_cells) <= 1:\n",
    "             continue\n",
    "        # first column in table\n",
    "        ticker = row_cells[0].text.strip()\n",
    "        company_list.append(ticker)\n",
    "        \n",
    "    # dictionary of financial metrics   \n",
    "    return company_list\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = get_tickers()\n",
    "print(f\"{len(ticker_list)} tickers in list\")\n",
    "print(ticker_list[:5], ticker_list[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gets data for first 10 on a single thread\n",
    "t0 = time.time()\n",
    "company_data_list = []\n",
    "for ticker in ticker_list[:10]:\n",
    "    company_data_list.append(get_company_data(ticker))\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"{:.4} seconds\".format(t1-t0))\n",
    "print(f\"{len(company_data_list)} tickers in list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make data frame from list of dictionaries\n",
    "df = pd.DataFrame(company_data_list)    \n",
    "                            \n",
    "#df = pd.DataFrame(list(map(get_company_data, ticker_list_filtered)))\n",
    "                        \n",
    "df = df.set_index('ticker')\n",
    "print(f\"{len(df.index)} Rows, {len(df.columns)} Columns\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"company_data_list length: {len(company_data_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, f in enumerate(os.listdir()):\n",
    "    print(index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "df.to_csv('financial_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, f in enumerate(os.listdir()):\n",
    "    print(index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a csv\n",
    "main_str = \"\"\n",
    "\n",
    "main_str += (df.index.name).replace(\",\",\" \")\n",
    "main_str += \",\"\n",
    "\n",
    "# header row\n",
    "for index, col_name in enumerate(df.columns):\n",
    "    main_str += col_name.replace(\",\",\" \")\n",
    "    if index == (len(df.columns)-1):\n",
    "        main_str += \"\\n\"\n",
    "    else:\n",
    "        main_str += \",\"\n",
    "        \n",
    "# body rows               \n",
    "for row in range(len(df.index)):\n",
    "    main_str += df.index[row] + \",\"\n",
    "    for col in range(len(df.columns)):  \n",
    "        main_str += (df.iloc[row, col]).replace(\",\",\" \")\n",
    "        if col == (len(df.columns)-1):\n",
    "            main_str += \"\\n\"\n",
    "        else:\n",
    "            main_str += \",\"\n",
    "            \n",
    "with open('made_csv2.csv', 'w') as file:\n",
    "    file.write(main_str)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithread Requests (Thread pool executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(ticker_list)} tickers in list\")\n",
    "print(ticker_list[:5], ticker_list[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asynchronously call \"get_company_data\"\n",
    "def thread_function(num: int, ticker_list: list=ticker_list, get_company_data:'function'=get_company_data) -> None:\n",
    "    # ticker at point in list\n",
    "    ticker = ticker_list[num]\n",
    "    # calls company data function\n",
    "    company_data = get_company_data(ticker)\n",
    "    # adds compnay data to shared list\n",
    "    company_data_list.append(company_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "company_data_list = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        executor.map(thread_function, range(30))\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"{:.4} seconds\".format(t1-t0))\n",
    "print(f\"{len(company_data_list)} tickers in list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(company_data_list)\n",
    "df = df.set_index('ticker')\n",
    "print(f\"{len(df.index)} Rows, {len(df.columns)} Columns\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "df.to_csv('financial_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = []\n",
    "values = []\n",
    "for index, i in df.iterrows():\n",
    "    pf = float(i[\"Profit Margin \"].replace('%',''))\n",
    "#     company = i[\"!!company_name\"].strip()\n",
    "#     labels.append(company)\n",
    "    \n",
    "    labels.append(index)\n",
    "    values.append(pf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.bar(labels, values)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Profit margin (%)')\n",
    "plt.title('Profit margin by company')\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Following sections will not work with Anaconda alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using An In Memory Data Base (Redis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must start redis server in terminal first\n",
    "r_db = redis.Redis(port=6377, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_db.mset({\"name\": \"Stefan\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key value from db\n",
    "r_db.mget('name')[0].decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearS db\n",
    "r_db.flushall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(ticker_list)} tickers in list\")\n",
    "print(ticker_list[:5], ticker_list[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asynchronously call \"get_company_data\"\n",
    "def thread_map(num, ticker_list=ticker_list, get_company_data=get_company_data):\n",
    "    # ticker at point in list\n",
    "    ticker = ticker_list[num]\n",
    "    # calls company data function\n",
    "    company_data = get_company_data(ticker)\n",
    "    r_db.mset({ticker: str(company_data)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        executor.map(thread_map, range(40))\n",
    "        \n",
    "t1 = time.time()\n",
    "print(\"{:.4} seconds\".format(t1-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create datadrame from data in redis db\n",
    "df = pd.DataFrame([json.loads(r_db.get(ticker).decode('UTF-8').replace(\"'\",'\"')) for ticker in r_db.keys()]).set_index('ticker')\n",
    "\n",
    "print(f\"{len(df.index)} Rows, {len(df.columns)} Columns\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.wsj.com/news/archive/20021001', timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = bs.BeautifulSoup(response.text)\n",
    "# print(source.text)\n",
    "articles = source.find_all('article')\n",
    "print(len(articles))\n",
    "for article in articles:\n",
    "    print(article.text)\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium (render full web page before extracting data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "# for setting request headers\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "# selecting items from dropdown list\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define webdriver use firefox browser\n",
    "options = Options()\n",
    "# run without broswer window\n",
    "#options.add_argument('--headless')\n",
    "\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.wsj.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.wsj.com/news/archive/20040608')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get newspaper title and article summary from WSJ archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of dates for wsj archive url\n",
    "def create_date() -> list:\n",
    "    start_date = datetime.date(1996, 4, 6)\n",
    "    dates_list = []\n",
    "    while True:\n",
    "        start_date += datetime.timedelta(days=1)\n",
    "        dates_list.append(str(start_date).replace('-',''))\n",
    "        if datetime.date.today() == start_date:\n",
    "            break\n",
    "    return dates_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list list of possible dates fro url\n",
    "dates = create_date()\n",
    "print(f\"{len(dates)} total dates\")\n",
    "print(dates[1:3],dates[-3:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current directory\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes directory for each days csvs\n",
    "if not os.path.exists('wsj_csvs'):\n",
    "    os.mkdir('wsj_csvs')\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes errors directory for each days csvs\n",
    "if not os.path.exists('errors'):\n",
    "    os.mkdir('errors')\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_db = redis.Redis(port=6377, db=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a formatted date, appends to wsj archive url, returns df of days articles\n",
    "def get_days_news(date):\n",
    "\n",
    "    driver.get(f'https://www.wsj.com/news/archive/{date}')\n",
    "    raw_source = driver.page_source\n",
    "    source = bs.BeautifulSoup(raw_source)\n",
    "    articles = source.select(\"article[class*='WSJTheme--story']\")\n",
    "    \n",
    "    # if page does not load, date is added to error file\n",
    "    timeout = 0\n",
    "    while len(articles) == 0:\n",
    "        time.sleep(1)\n",
    "        raw_source = driver.page_source\n",
    "        source = bs.BeautifulSoup(raw_source)\n",
    "        articles = source.select(\"article[class*='WSJTheme--story']\")\n",
    "        timeout += 1\n",
    "        if timeout >= 10:\n",
    "            with open(os.path.join(os.getcwd(), 'errors', f\"{date}.txt\"),'w') as f:\n",
    "                f.write(date)\n",
    "            return\n",
    "    \n",
    "    time.sleep(1)\n",
    "        \n",
    "    dict_list = []\n",
    "    for article in articles:\n",
    "        #print(article.text)\n",
    "        \n",
    "        # the tree sections of each article row\n",
    "        days_articles = {'section': article.select(\"div[class*='WSJTheme--flashline']\"),\n",
    "                         'headline': article.select(\"h3[class*='WSJTheme--headline']\"), \n",
    "                         'summary': article.select(\"p[class*='WSJTheme--summary']\")\n",
    "                        }\n",
    "        \n",
    "        # adds each of the three sections to dict, used for df\n",
    "        for item in days_articles:\n",
    "            if days_articles[item] == []:\n",
    "                days_articles[item] = 'None'\n",
    "            else:\n",
    "                days_articles[item] = days_articles[item][0].text\n",
    "         \n",
    "        # for date columns\n",
    "        days_articles['date'] = date\n",
    "        \n",
    "        dict_list.append(days_articles)\n",
    "    \n",
    "     # writes to redis db\n",
    "    r_db.mset({str(date): str(dict_list)})\n",
    "    \n",
    "    # creates pandas df from list of article dicts\n",
    "    df = pd.DataFrame(dict_list)\n",
    "    \n",
    "    # add to csv\n",
    "    df.to_csv(os.path.join(os.getcwd(), 'wsj_csvs', f\"{date}.csv\"))\n",
    "    \n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets archive at specific day\n",
    "df = get_days_news(dates[1005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates[4000:4005]:\n",
    "    get_days_news(date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 70)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('alert(\"warning\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('console.log(\"selenium\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = driver.execute_script('let f = document.querySelectorAll(\"div\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
